# name: test/sql/local/irc_any_catalog/insert/test_insert_into_table_incorrect_types.test
# description: test integration with iceberg catalog read
# group: [insert]

require-env ICEBERG_SERVER_AVAILABLE

require-env SECRETS_CREATED_AND_CATALOG_ATTACHED

require avro

require parquet

require iceberg

require httpfs

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
CALL enable_logging('HTTP');

statement ok
set logging_level='debug';

statement ok
create schema if not exists my_datalake.default;

statement ok
drop table if exists my_datalake.default.test;

# we can create a table with invalid iceberg types, but they get converted to valid iceberg types.
statement error
create table my_datalake.default.test (int_col UTINYINT, long_col UINT32, string_col varchar);
----
<REGEX>:.*Column type UTINYINT is not a valid Iceberg Type.*

statement ok
create table my_datalake.default.test (int_col INT, long_col LONG, string_col varchar);

query II
select column_name, column_type from (describe from my_datalake.default.test);
----
int_col	INTEGER
long_col	BIGINT
string_col	VARCHAR

# try inserting uint32 into integer column
statement error
insert into my_datalake.default.test values (4294967295::UINT32, null, null);
----
<REGEX>:.*Conversion Error.*out of range for the destination type INT32.*

# insert a tinytint
statement ok
insert into my_datalake.default.test values (8::TINYINT, null, null);

# there is only 1 datafile right now, set it as a variable.
statement ok
set variable data_file = (select file_path  from iceberg_metadata(my_datalake.default.test) limit 1);

# verify file is a parquet file
query I
select count(*) from (select getvariable('data_file') file_name where file_name like '%.parquet');
----
1

# query the column_type of the parquet file, make sure it is integer and not tinyint.
query I
select column_type from (describe (from read_parquet(getvariable('data_file')))) where column_name = 'int_col';
----
INTEGER

# add another line where 343 is auto cast to varchar
statement ok
insert into my_datalake.default.test values (null, null, 343);

# get the second data file
statement ok
set variable data_file_2 = (select file_path from iceberg_metadata(my_datalake.default.test) where manifest_sequence_number = 2);

# query the column_type of the parquet file, make sure it is varchar and not int.
query I
select column_type from (describe (from read_parquet(getvariable('data_file_2')))) where column_name = 'string_col';
----
VARCHAR

# add another line where 4294967295 is auto cast to BIGINT
statement ok
insert into my_datalake.default.test values (null, 4294967295::UINT32, null);

# get the third data file
statement ok
set variable data_file_3 = (select file_path from iceberg_metadata(my_datalake.default.test) where manifest_sequence_number = 3);

# query the column_type of the parquet file, make sure it is BIGINT and not UINT32
query I
select column_type from (describe (from read_parquet(getvariable('data_file_3')))) where column_name = 'long_col';
----
BIGINT

query III
select * from my_datalake.default.test order by all;
----
8	NULL	NULL
NULL	4294967295	NULL
NULL	NULL	343


statement ok
drop table if exists my_datalake.default.test_upcasting;

statement ok
create table my_datalake.default.test_upcasting (int_col int, long_col BIGINT);

statement ok
insert into my_datalake.default.test_upcasting select usmallint, usmallint from test_all_types();

statement ok
insert into my_datalake.default.test_upcasting select utinyint, utinyint from test_all_types();

statement ok
insert into my_datalake.default.test_upcasting select tinyint, tinyint from test_all_types();

statement ok
insert into my_datalake.default.test_upcasting select smallint, smallint from test_all_types();

statement ok
insert into my_datalake.default.test_upcasting select NULL, uint from test_all_types();

statement error
insert into my_datalake.default.test_upcasting select NULL, ubigint from test_all_types();
----
<REGEX>:.*Conversion Error.*out of range for the destination type INT64.*



