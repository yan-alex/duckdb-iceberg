# name: test/sql/local/irc_any_catalog/insert/test_read_upper_and_lower_bounds.test
# group: [insert]

require-env ICEBERG_SERVER_AVAILABLE

require-env SECRETS_CREATED_AND_CATALOG_ATTACHED

require avro

require parquet

require iceberg

require httpfs

require icu

require core_functions

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
set enable_logging=true

statement ok
set logging_level='debug'

statement ok
SET TimeZone = 'UTC';

statement ok
pragma threads=1;


statement ok
set variable manifest_path_spark = (select manifest_path from iceberg_metadata(my_datalake.default.spark_written_upper_lower_bounds) order by manifest_sequence_number desc limit 1);

# This is a test to make sure that spark only writes one data file for the given set up for the test
query IIII
from (select unnest(map_keys(lower_bounds)) lower_keys, unnest(map_values(lower_bounds)) lower_vals, unnest(map_keys(upper_bounds)) upper_keys, unnest(map_values(upper_bounds)) upper_vals from (select unnest(data_file) from read_avro(getvariable('manifest_path_spark')))) where lower_keys in (1, 2, 3, 5, 6, 7, 8, 9) order by all;
----
1	\x00\x00\x00\x80	1	\xFF\xFF\xFF\x7F
2	\x00\x00\x00\x00\x00\x00\x00\x80	2	\xFF\xFF\xFF\xFF\xFF\xFF\xFF\x7F
3	(empty)	3	ZZZZZZZZZZZZZZZ[
5	\xFF\xFF\x7F\xFF	5	\xFF\xFF\x7F\x7F
6	\xFF\xFF\xFF\xFF\xFF\xFF\xEF\xFF	6	\xFF\xFF\xFF\xFF\xFF\xFF\xEF\x7F
7	\xDCy\x0D\x90?\x00\x01	7	#\x86\xF2o\xC0\xFF\xFF
8	\xC6\x06\xF5\xFF	8	\xA0\xC0,\x00
9	\x00@\xD4\x00\x01@#\xFF	9	\xFF_s\xCC\x0CD\x84\x03