#include "storage/iceberg_insert.hpp"
#include "storage/catalog/iceberg_catalog.hpp"
#include "storage/catalog/iceberg_table_entry.hpp"
#include "storage/iceberg_table_information.hpp"
#include "metadata/iceberg_column_definition.hpp"
#include "iceberg_multi_file_list.hpp"
#include "storage/iceberg_transaction.hpp"
#include "iceberg_value.hpp"
#include "utils/iceberg_type.hpp"
#include "duckdb/catalog/catalog_entry/copy_function_catalog_entry.hpp"
#include "duckdb/main/client_data.hpp"
#include "duckdb/planner/operator/logical_copy_to_file.hpp"
#include "duckdb/execution/physical_operator_states.hpp"
#include "duckdb/planner/operator/logical_insert.hpp"
#include "duckdb/planner/operator/logical_create_table.hpp"
#include "duckdb/planner/parsed_data/bound_create_table_info.hpp"
#include "duckdb/execution/operator/projection/physical_projection.hpp"
#include "duckdb/execution/operator/scan/physical_table_scan.hpp"
#include "duckdb/common/types/uuid.hpp"

namespace duckdb {

IcebergInsert::IcebergInsert(PhysicalPlan &physical_plan, LogicalOperator &op, TableCatalogEntry &table,
                             physical_index_vector_t<idx_t> column_index_map_p)
    : PhysicalOperator(physical_plan, PhysicalOperatorType::EXTENSION, op.types, 1), table(&table), schema(nullptr),
      column_index_map(std::move(column_index_map_p)) {
}

IcebergInsert::IcebergInsert(PhysicalPlan &physical_plan, LogicalOperator &op, SchemaCatalogEntry &schema,
                             unique_ptr<BoundCreateTableInfo> info)
    : PhysicalOperator(physical_plan, PhysicalOperatorType::EXTENSION, op.types, 1), table(nullptr), schema(&schema),
      info(std::move(info)) {
}

IcebergInsert::IcebergInsert(PhysicalPlan &physical_plan, const vector<LogicalType> &types, TableCatalogEntry &table)
    : PhysicalOperator(physical_plan, PhysicalOperatorType::EXTENSION, types, 1), table(&table), schema(nullptr) {
}

IcebergCopyInput::IcebergCopyInput(ClientContext &context, IcebergTableEntry &table)
    : catalog(table.catalog.Cast<IcebergCatalog>()), columns(table.GetColumns()) {
	data_path = table.table_info.table_metadata.GetDataPath();
}

IcebergCopyInput::IcebergCopyInput(ClientContext &context, IcebergSchemaEntry &schema, const ColumnList &columns,
                                   const string &data_path_p)
    : catalog(schema.catalog.Cast<IcebergCatalog>()), columns(columns) {
	// When data_path_p is provided directly, it's already the table location
	// We should check if it has write.data.path property, but since this is a schema-level
	// constructor and we don't have access to table metadata, we use the default behavior
	data_path = data_path_p + "/data";
}

IcebergInsertGlobalState::IcebergInsertGlobalState(ClientContext &context)
    : GlobalSinkState(), context(context), insert_count(0) {
}

unique_ptr<GlobalSinkState> IcebergInsert::GetGlobalSinkState(ClientContext &context) const {
	return make_uniq<IcebergInsertGlobalState>(context);
}

//===--------------------------------------------------------------------===//
// Sink
//===--------------------------------------------------------------------===//
static string ParseQuotedValue(const string &input, idx_t &pos) {
	if (pos >= input.size() || input[pos] != '"') {
		throw InvalidInputException("Failed to parse quoted value - expected a quote");
	}
	string result;
	pos++;
	for (; pos < input.size(); pos++) {
		if (input[pos] == '"') {
			pos++;
			// check if this is an escaped quote
			if (pos < input.size() && input[pos] == '"') {
				// escaped quote
				result += '"';
				continue;
			}
			return result;
		}
		result += input[pos];
	}
	throw InvalidInputException("Failed to parse quoted value - unterminated quote");
}

static vector<string> ParseQuotedList(const string &input, char list_separator) {
	vector<string> result;
	if (input.empty()) {
		return result;
	}
	idx_t pos = 0;
	while (true) {
		result.push_back(ParseQuotedValue(input, pos));
		if (pos >= input.size()) {
			break;
		}
		if (input[pos] != list_separator) {
			throw InvalidInputException("Failed to parse list - expected a %s", string(1, list_separator));
		}
		pos++;
	}
	return result;
}

static void AddToColDefMap(case_insensitive_map_t<optional_ptr<IcebergColumnDefinition>> &name_to_coldef,
                           string col_name_prefix, optional_ptr<IcebergColumnDefinition> column_def) {
	string column_name = column_def->name;
	if (!col_name_prefix.empty()) {
		column_name = col_name_prefix + "." + column_def->name;
	}
	if (column_def->IsIcebergPrimitiveType()) {
		name_to_coldef.emplace(column_name, column_def.get());
	} else {
		for (auto &child : column_def->children) {
			AddToColDefMap(name_to_coldef, column_name, child.get());
		}
	}
}

IcebergColumnStats IcebergInsert::ParseColumnStats(const LogicalType &type, const vector<Value> &col_stats,
                                                   ClientContext &context) {
	IcebergColumnStats column_stats(type);
	for (idx_t stats_idx = 0; stats_idx < col_stats.size(); stats_idx++) {
		auto &stats_children = StructValue::GetChildren(col_stats[stats_idx]);
		auto &stats_name = StringValue::Get(stats_children[0]);
		if (stats_name == "min") {
			D_ASSERT(!column_stats.has_min);
			column_stats.min = StringValue::Get(stats_children[1]);
			column_stats.has_min = true;
		} else if (stats_name == "max") {
			D_ASSERT(!column_stats.has_max);
			column_stats.max = StringValue::Get(stats_children[1]);
			column_stats.has_max = true;
		} else if (stats_name == "null_count") {
			D_ASSERT(!column_stats.has_null_count);
			column_stats.has_null_count = true;
			column_stats.null_count = StringUtil::ToUnsigned(StringValue::Get(stats_children[1]));
		} else if (stats_name == "column_size_bytes") {
			column_stats.has_column_size_bytes = true;
			column_stats.column_size_bytes = StringUtil::ToUnsigned(StringValue::Get(stats_children[1]));
		} else if (stats_name == "has_nan") {
			column_stats.has_contains_nan = true;
			column_stats.contains_nan = StringValue::Get(stats_children[1]) == "true";
		} else {
			// Ignore other stats types.s
			DUCKDB_LOG_INFO(context, StringUtil::Format("Did not write column stats %s", stats_name));
		}
	}
	return column_stats;
}

static bool IsMapType(string col_name, IcebergTableSchema &table_schema) {
	for (auto &col : table_schema.columns) {
		if (col->name == col_name) {
			if (col->type.id() == LogicalTypeId::MAP) {
				return true;
			}
		}
	}
	return false;
}

void IcebergInsert::AddWrittenFiles(IcebergInsertGlobalState &global_state, DataChunk &chunk,
                                    optional_ptr<TableCatalogEntry> table) {
	D_ASSERT(table);
	// grab lock for written files vector
	lock_guard<mutex> guard(global_state.lock);
	auto &ic_table = table->Cast<IcebergTableEntry>();
	auto partition_id = ic_table.table_info.table_metadata.default_spec_id;
	for (idx_t r = 0; r < chunk.size(); r++) {
		IcebergManifestEntry manifest_entry;
		manifest_entry.status = IcebergManifestEntryStatusType::ADDED;
		if (partition_id) {
			manifest_entry.partition_spec_id = static_cast<int32_t>(partition_id);
		} else {
			manifest_entry.partition_spec_id = 0;
		}

		auto &data_file = manifest_entry.data_file;
		data_file.file_path = chunk.GetValue(0, r).GetValue<string>();
		data_file.record_count = static_cast<int64_t>(chunk.GetValue(1, r).GetValue<idx_t>());
		data_file.file_size_in_bytes = static_cast<int64_t>(chunk.GetValue(2, r).GetValue<idx_t>());
		data_file.content = IcebergManifestEntryContentType::DATA;
		data_file.file_format = "parquet";

		// extract the column stats
		auto column_stats = chunk.GetValue(4, r);
		auto &map_children = MapValue::GetChildren(column_stats);

		global_state.insert_count += data_file.record_count;

		auto table_current_schema_id = ic_table.table_info.table_metadata.current_schema_id;
		auto ic_schema = ic_table.table_info.table_metadata.schemas[table_current_schema_id];

		case_insensitive_map_t<optional_ptr<IcebergColumnDefinition>> column_info;
		for (auto &column : ic_schema->columns) {
			AddToColDefMap(column_info, "", column.get());
		}

		for (idx_t col_idx = 0; col_idx < map_children.size(); col_idx++) {
			auto &struct_children = StructValue::GetChildren(map_children[col_idx]);
			auto &col_name = StringValue::Get(struct_children[0]);
			auto &col_stats = MapValue::GetChildren(struct_children[1]);
			auto column_names = ParseQuotedList(col_name, '.');
			auto normalized_col_name = StringUtil::Join(column_names, ".");

			auto ic_column_info_it = column_info.find(normalized_col_name);
			D_ASSERT(ic_column_info_it != column_info.end());
			auto column_info_stats = ic_column_info_it->second;
			auto stats = ParseColumnStats(column_info_stats->type, col_stats, global_state.context);

			// a map type cannot violate not null constraints.
			// Null value counts can be off since an empty map is the same as a null map.
			bool is_map = IsMapType(column_names[0], *ic_schema);
			if (!is_map && column_info_stats->required && stats.has_null_count && stats.null_count > 0) {
				throw ConstraintException("NOT NULL constraint failed: %s.%s", table->name, normalized_col_name);
			}
			// go through stats and add upper and lower bounds
			// Do serialization of values here in case we read transaction updates
			if (stats.has_min) {
				auto serialized_value =
				    IcebergValue::SerializeValue(stats.min, column_info_stats->type, SerializeBound::LOWER_BOUND);
				if (serialized_value.HasError()) {
					throw InvalidConfigurationException(serialized_value.GetError());
				} else if (serialized_value.HasValue()) {
					data_file.lower_bounds[column_info_stats->id] = serialized_value.GetValue();
				}
			}
			if (stats.has_max) {
				auto serialized_value =
				    IcebergValue::SerializeValue(stats.max, column_info_stats->type, SerializeBound::UPPER_BOUND);
				if (serialized_value.HasError()) {
					throw InvalidConfigurationException(serialized_value.GetError());
				} else if (serialized_value.HasValue()) {
					data_file.upper_bounds[column_info_stats->id] = serialized_value.GetValue();
				}
			}
			if (stats.has_column_size_bytes) {
				data_file.column_sizes[column_info_stats->id] = stats.column_size_bytes;
			}
			if (stats.has_null_count) {
				data_file.null_value_counts[column_info_stats->id] = stats.null_count;
			}

			//! nan_value_counts won't work, we can only indicate if they exist.
			//! TODO: revisit when duckdb/duckdb can record nan_value_counts
		}

		//! TODO: extract the partition info
		// auto partition_info = chunk.GetValue(5, r);
		// if (!partition_info.IsNull()) {
		//	auto &partition_children = MapValue::GetChildren(partition_info);
		//	for (idx_t col_idx = 0; col_idx < partition_children.size(); col_idx++) {
		//		auto &struct_children = StructValue::GetChildren(partition_children[col_idx]);
		//		auto &part_value = StringValue::Get(struct_children[1]);

		//		IcebergPartition file_partition_info;
		//		file_partition_info.partition_column_idx = col_idx;
		//		file_partition_info.partition_value = part_value;
		//		data_file.partition_values.push_back(std::move(file_partition_info));
		//	}
		//}

		global_state.written_files.push_back(std::move(manifest_entry));
	}
}

SinkResultType IcebergInsert::Sink(ExecutionContext &context, DataChunk &chunk, OperatorSinkInput &input) const {
	auto &global_state = input.global_state.Cast<IcebergInsertGlobalState>();

	// TODO: pass through the partition id?
	AddWrittenFiles(global_state, chunk, table);

	return SinkResultType::NEED_MORE_INPUT;
}

//===--------------------------------------------------------------------===//
// GetData
//===--------------------------------------------------------------------===//
SourceResultType IcebergInsert::GetDataInternal(ExecutionContext &context, DataChunk &chunk,
                                                OperatorSourceInput &input) const {
	auto &global_state = sink_state->Cast<IcebergInsertGlobalState>();
	auto value = Value::BIGINT(global_state.insert_count);
	chunk.SetCardinality(1);
	chunk.SetValue(0, 0, value);
	return SourceResultType::FINISHED;
}

//===--------------------------------------------------------------------===//
// Finalize
//===--------------------------------------------------------------------===//
SinkFinalizeType IcebergInsert::Finalize(Pipeline &pipeline, Event &event, ClientContext &context,
                                         OperatorSinkFinalizeInput &input) const {
	auto &global_state = input.global_state.Cast<IcebergInsertGlobalState>();

	auto &irc_table = table->Cast<IcebergTableEntry>();
	auto &table_info = irc_table.table_info;
	auto &transaction = IcebergTransaction::Get(context, table->catalog);
	auto &iceberg_transaction = transaction.Cast<IcebergTransaction>();

	lock_guard<mutex> guard(global_state.lock);
	if (!global_state.written_files.empty()) {
		ApplyTableUpdate(table_info, iceberg_transaction, [&](IcebergTableInformation &tbl) {
			tbl.AddSnapshot(transaction, std::move(global_state.written_files));
		});
	}
	return SinkFinalizeType::READY;
}

//===--------------------------------------------------------------------===//
// Helpers
//===--------------------------------------------------------------------===//
string IcebergInsert::GetName() const {
	D_ASSERT(table);
	return "ICEBERG_INSERT";
}

InsertionOrderPreservingMap<string> IcebergInsert::ParamsToString() const {
	InsertionOrderPreservingMap<string> result;
	result["Table Name"] = table ? table->name : info->Base().table;
	return result;
}

//===--------------------------------------------------------------------===//
// Plan
//===--------------------------------------------------------------------===//
static optional_ptr<CopyFunctionCatalogEntry> TryGetCopyFunction(DatabaseInstance &db, const string &name) {
	D_ASSERT(!name.empty());
	auto &system_catalog = Catalog::GetSystemCatalog(db);
	auto data = CatalogTransaction::GetSystemTransaction(db);
	auto &schema = system_catalog.GetSchema(data, DEFAULT_SCHEMA);
	return schema.GetEntry(data, CatalogType::COPY_FUNCTION_ENTRY, name)->Cast<CopyFunctionCatalogEntry>();
}

static Value GetFieldIdValue(const IcebergColumnDefinition &column) {
	auto column_value = Value::BIGINT(column.id);
	if (column.children.empty()) {
		// primitive type - return the field-id directly
		return column_value;
	}
	// nested type - generate a struct and recurse into children
	child_list_t<Value> values;
	values.emplace_back("__duckdb_field_id", std::move(column_value));
	for (auto &child : column.children) {
		values.emplace_back(child->name, GetFieldIdValue(*child));
	}
	return Value::STRUCT(std::move(values));
}

static Value WrittenFieldIds(const IcebergTableSchema &schema) {
	auto &columns = schema.columns;

	child_list_t<Value> values;
	for (idx_t c_idx = 0; c_idx < columns.size(); c_idx++) {
		auto &column = columns[c_idx];
		values.emplace_back(column->name, GetFieldIdValue(*column));
	}
	return Value::STRUCT(std::move(values));
}

unique_ptr<CopyInfo> GetBindInput(IcebergCopyInput &input) {
	// Create Copy Info
	auto info = make_uniq<CopyInfo>();
	info->file_path = input.data_path;
	info->format = "parquet";
	info->is_from = false;
	for (auto &option : input.options) {
		info->options[option.first] = option.second;
	}
	return info;
}

vector<IcebergManifestEntry> IcebergInsert::GetInsertManifestEntries(IcebergInsertGlobalState &global_state) {
	lock_guard<mutex> guard(global_state.lock);
	return std::move(global_state.written_files);
}

PhysicalOperator &IcebergInsert::PlanCopyForInsert(ClientContext &context, PhysicalPlanGenerator &planner,
                                                   IcebergCopyInput &copy_input, optional_ptr<PhysicalOperator> plan) {
	// Get Parquet Copy function
	auto copy_fun = TryGetCopyFunction(*context.db, "parquet");
	if (!copy_fun) {
		throw MissingExtensionException("Did not find parquet copy function required to write to iceberg table");
	}

	auto names_to_write = copy_input.columns.GetColumnNames();
	auto types_to_write = copy_input.columns.GetColumnTypes();

	auto wat = GetBindInput(copy_input);
	auto bind_input = CopyFunctionBindInput(*wat);

	auto function_data = copy_fun->function.copy_to_bind(context, bind_input, names_to_write, types_to_write);

	auto &physical_copy = planner.Make<PhysicalCopyToFile>(
	    GetCopyFunctionReturnLogicalTypes(CopyFunctionReturnType::WRITTEN_FILE_STATISTICS), copy_fun->function,
	    std::move(function_data), 1);
	auto &physical_copy_ref = physical_copy.Cast<PhysicalCopyToFile>();

	vector<idx_t> partition_columns;
	//! TODO: support partitions
	// auto partitions = op.table.Cast<IcebergTableEntry>().snapshot->GetPartitionColumns();
	// if (partitions.size() != 0) {
	//	auto column_names = op.table.Cast<IcebergTableEntry>().GetColumns().GetColumnNames();
	//	for (int64_t i = 0; i < partitions.size(); i++) {
	//		for (int64_t j = 0; j < column_names.size(); j++) {
	//			if (column_names[j] == partitions[i]) {
	//				partition_columns.push_back(j);
	//				break;
	//			}
	//		}
	//	}
	//}

	physical_copy_ref.use_tmp_file = false;
	if (!partition_columns.empty()) {
		physical_copy_ref.filename_pattern.SetFilenamePattern("{uuidv7}");
		physical_copy_ref.file_path = copy_input.data_path;
		physical_copy_ref.partition_output = true;
		physical_copy_ref.partition_columns = partition_columns;
		physical_copy_ref.write_empty_file = true;
		physical_copy_ref.rotate = false;
	} else {
		physical_copy_ref.filename_pattern.SetFilenamePattern("{uuidv7}");
		physical_copy_ref.file_path = copy_input.data_path;
		physical_copy_ref.partition_output = false;
		physical_copy_ref.write_empty_file = false;
		physical_copy_ref.file_size_bytes = IcebergCatalog::DEFAULT_TARGET_FILE_SIZE;
		physical_copy_ref.rotate = true;
	}

	physical_copy_ref.file_extension = "parquet";
	physical_copy_ref.overwrite_mode = CopyOverwriteMode::COPY_OVERWRITE_OR_IGNORE;
	physical_copy_ref.per_thread_output = false;
	physical_copy_ref.return_type = CopyFunctionReturnType::WRITTEN_FILE_STATISTICS; // TODO: capture stats
	physical_copy_ref.write_partition_columns = true;
	if (plan) {
		physical_copy.children.push_back(*plan);
	}
	physical_copy_ref.names = names_to_write;
	physical_copy_ref.expected_types = types_to_write;
	physical_copy_ref.hive_file_pattern = true;
	return physical_copy;
}

void VerifyDirectInsertionOrder(LogicalInsert &op) {
	idx_t column_index = 0;
	for (auto &mapping : op.column_index_map) {
		if (mapping == DConstants::INVALID_INDEX || mapping != column_index) {
			//! See issue#444
			throw NotImplementedException("Iceberg inserts don't support targeted inserts yet (i.e tbl(col1,col2))");
		}
		column_index++;
	}
}

PhysicalOperator &IcebergInsert::PlanInsert(ClientContext &context, PhysicalPlanGenerator &planner,
                                            IcebergTableEntry &table) {
	optional_idx partition_id;
	vector<LogicalType> return_types;
	// the one return value is how many rows we are inserting
	return_types.emplace_back(LogicalType::BIGINT);
	return planner.Make<IcebergInsert>(return_types, table);
}

PhysicalOperator &IcebergCatalog::PlanInsert(ClientContext &context, PhysicalPlanGenerator &planner, LogicalInsert &op,
                                             optional_ptr<PhysicalOperator> plan) {
	if (op.return_chunk) {
		throw BinderException("RETURNING clause not yet supported for insertion into Iceberg table");
	}

	if (op.on_conflict_info.action_type != OnConflictAction::THROW) {
		throw BinderException("ON CONFLICT clause not yet supported for insertion into Iceberg table");
	}

	VerifyDirectInsertionOrder(op);

	auto &table_entry = op.table.Cast<IcebergTableEntry>();
	// FIXME: Inserts into V3 tables is not yet supported since
	// we need to keep track of row lineage, which we do not support
	// https://iceberg.apache.org/spec/#row-lineage
	if (table_entry.table_info.table_metadata.iceberg_version == 3) {
		throw NotImplementedException("Insert into Iceberg V3 tables");
	}
	table_entry.PrepareIcebergScanFromEntry(context);
	auto &table_info = table_entry.table_info;
	auto &schema = table_info.table_metadata.GetLatestSchema();

	auto &partition_spec = table_info.table_metadata.GetLatestPartitionSpec();
	if (!partition_spec.IsUnpartitioned()) {
		throw NotImplementedException("INSERT into a partitioned table is not supported yet");
	}
	if (table_info.table_metadata.HasSortOrder()) {
		auto &sort_spec = table_info.table_metadata.GetLatestSortOrder();
		if (sort_spec.IsSorted()) {
			throw NotImplementedException("INSERT into a sorted iceberg table is not supported yet");
		}
	}

	// Create Copy Info
	auto info = make_uniq<IcebergCopyInput>(context, table_entry);

	vector<Value> field_input;
	field_input.push_back(WrittenFieldIds(schema));
	info->options["field_ids"] = std::move(field_input);

	auto &insert = planner.Make<IcebergInsert>(op, op.table, op.column_index_map);

	auto &physical_copy = IcebergInsert::PlanCopyForInsert(context, planner, *info, plan);

	insert.children.push_back(physical_copy);

	return insert;
}

PhysicalOperator &IcebergCatalog::PlanCreateTableAs(ClientContext &context, PhysicalPlanGenerator &planner,
                                                    LogicalCreateTable &op, PhysicalOperator &plan) {
	// TODO: parse partition information here.
	auto &schema = op.schema;

	auto &ic_schema_entry = schema.Cast<IcebergSchemaEntry>();
	auto &catalog = ic_schema_entry.catalog;
	auto transaction = catalog.GetCatalogTransaction(context);

	// create the table. Takes care of committing to rest catalog and getting the metadata location etc.
	// setting the schema
	auto table = ic_schema_entry.CreateTable(transaction, context, *op.info);
	if (!table) {
		throw InternalException("Table could not be created");
	}
	auto &ic_table = table->Cast<IcebergTableEntry>();
	// We need to load table credentials into our secrets for when we copy files
	ic_table.PrepareIcebergScanFromEntry(context);

	auto &table_schema = ic_table.table_info.table_metadata.GetLatestSchema();

	// Create Copy Info
	auto info = make_uniq<IcebergCopyInput>(context, ic_table);

	vector<Value> field_input;
	field_input.push_back(WrittenFieldIds(table_schema));
	info->options["field_ids"] = std::move(field_input);

	auto &physical_copy = IcebergInsert::PlanCopyForInsert(context, planner, *info, plan);
	physical_index_vector_t<idx_t> column_index_map;
	auto &insert = planner.Make<IcebergInsert>(op, ic_table, column_index_map);

	insert.children.push_back(physical_copy);
	return insert;
}

} // namespace duckdb
